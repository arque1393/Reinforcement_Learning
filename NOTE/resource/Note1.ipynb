{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6cefce",
   "metadata": {},
   "source": [
    "$\\Huge{\\texttt{abcdefghijklmanopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34545442",
   "metadata": {},
   "source": [
    "> The true action value for action is the expected reward : \n",
    "$\\large{\n",
    "\\texttt{q}(a) = \\mathbb{E}[R_{t}|A_{t}=a]\n",
    "}$ <br>\n",
    "A simple estimate is the average of the sample reward :<br>\n",
    "$\\large{\n",
    "Q_{t}(a)=}\\Large{\n",
    "\\frac{\\sum\\limits_{n=1}^{t}R_{n}\\mathcal{I}(A_{n}= a)}{\\sum\\limits_{n=1}^{t}\\mathcal{I}(A_{n}= a)}\n",
    "}$\n",
    "          Where <br>$\\large{\\mathcal{I}(True)=1}$ and <br>\n",
    "          $\\large{\\mathcal{I}(False)=0}$\n",
    "          \n",
    "     \n",
    "> Update Incrementally: <br>\n",
    "$\\large{\n",
    "Q_t(A_t)=Q_{t-1}(A_t)+\\alpha_{t}(R_t - Q_{t-1}(A_t))}$,<br>\n",
    "$\\large{ \\forall{a}\\neq A_t : Q_t(a)=Q_{t-1}(a)\n",
    "}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c90b98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c28505ef",
   "metadata": {},
   "source": [
    "#### Tutorial Playlist : \n",
    "https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151ddb1",
   "metadata": {},
   "source": [
    "I an MDP we have $\\large\\mathcal{S}$ set of States $\\large\\mathcal{A}$ set of Actions $\\large\\mathcal{R}$ set of reawards<br>\n",
    "t reperesents the time stamp $t=0,1,2,3,...$ <br>\n",
    "where ${S}_{t}\\in \\large\\mathcal{S}_{t}$ and ${A}_{t} \\in \\large\\mathcal{A}_{t}$ and ${R}_{t}\\in \\large\\mathcal{R}_{t}$ <br>\n",
    "\n",
    "Now we can assume that $R_{t+1}$ is a function of $S_{t}$ and $A_{t}$ <br>\n",
    "$\\large\\mathcal{f}(S_t, A_t)=R_{t+1}$<br>\n",
    "###### History\n",
    "History is a sequence of Satate Action Reward\n",
    "$\\large\\mathcal{H}= S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3 .....$<br>\n",
    "$\\large\\mathcal{H}_{t}= s_0,a_0,r_1,s_1,a_1,r_2,s_2,a_2,r_3 .....,s_{t-1},a_{t-1},r_t,s_t,$<br>\n",
    "![](resource/basicDiagram.png)\n",
    "\n",
    "#### Transition Probability \n",
    "$\\large\\mathcal{S}$ and $\\large\\mathcal{R}$ are the finite set of *randomvariable* $S_{t}$ and $R_{t}$ that are must to be follow some well known *Probability Distyribution*.<br>\n",
    "And Probability of current time step is related to previous time steps\n",
    "\n",
    "Such as let $s^\\prime \\in \\large\\mathcal{S}$ and $r \\in \\large\\mathcal{R}$ then $\\mathcal{P}(S_{t}=s^\\prime)$ and $\\mathcal{P}(R_{t}=r)$ are depend on $a \\in \\mathcal{A}(s)$ <br>\n",
    "\n",
    "\n",
    "$\\Large\\forall$  ${s} \\in \\large\\mathcal{S}_{t}$ and \n",
    "${s^\\prime} \\in \\large\\mathcal{S}_{t}$ and ${a} \\in \\large\\mathcal{A}_{t}$ and ${r} \\in \\large\\mathcal{R}_{t}$\n",
    "\n",
    "$\\large\\mathcal{P}({s^\\prime},a | s, a )=\\large\\mathcal{P}\\{S_t=s^\\prime, R_t-r | S_{t-1}=s,A_{t-1}=a \\}$\n",
    "\n",
    "\n",
    "### Return \n",
    "Return is denoted by $\\large{G}$ for each time step $t$ , $\\large{G_t}$ <br>\n",
    "$\\large{G_t}$ $={R_{t+1}}+{R_{t+2}}+{R_{t+3}}+{R_{t+4}}+.....+{R_{T}}$<br>\n",
    "Where $T$ is the final time step\n",
    "\n",
    "#### Discounted Return \n",
    "Discounted Return says that the return value are more dependent on currently collect reward and less depended on reward that are opccure comparetively more time ago\n",
    "\n",
    "$\\Large{G_t}$ $\\large={R_{t+1}}+\\mathcal{\\gamma}{R_{t+2}}+\\mathcal{\\gamma}^2{R_{t+3}}+\\mathcal{\\gamma}^3{R_{t+4}}+.....$<br>\n",
    "\n",
    "$\\Large{G_t}$ $=\\large{\\sum\\limits_{k=0}^{\\infty}\\mathcal{\\gamma}^{k}{R_{t+k+1}}}$\n",
    "\n",
    "$\\Large{G_t}$ $=\\large{R_{t+1}+\\mathcal{\\gamma}{G_{t+1}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d9c8b",
   "metadata": {},
   "source": [
    "### Policy and Value Functions \n",
    "If ann agent follows an polycy $\\Large{\\mathcal{\\pi}}$ at time $t$  then \n",
    " $\\Large{\\mathcal{\\pi}}(\\mathcal{a}|s) = \\large{\\mathcal{P}}( A_{t} = a | S_{t} = s )$\n",
    "\n",
    " #### Value Function\n",
    "\n",
    " A value function is is two types \n",
    " 1. **State-Value Function :** <br>\n",
    " A state Value function for Policy $\\Large\\mathcal\\pi$ is denoted as $\\Large{\\mathcal{v}_{\\pi}}$.<br>\n",
    " It is the measure of goodness of choosing the policy for any state $\\large{\\mathcal{\\pi}}$\n",
    "\n",
    "    $\\Large{\\mathcal{v}_{\\pi}}\\normalsize{(s)} = \\Large{\\mathbb{E}}\\normalsize{( G_{t} | S_t = s )}$ \n",
    "    \n",
    "    $\\Large{\\mathcal{v}_{\\pi}}\\normalsize{(s)} = \\Large{\\mathbb{E} (}\\normalsize{\\sum\\limits_{k=0}^{\\infty}\\mathcal{\\gamma}^{k}{R_{t+k+1}}}|S_{t}=s\\Large{)} $<br>   \n",
    "  \n",
    "    \n",
    "\n",
    " \n",
    " 2. **(State-Action) Value Function :** <br><br>\n",
    "   Action-Value Function is the measure of goodness choosing a policy against of any action for any state.<br>\n",
    "   It is denoted by $\\Large{\\mathcal{v}_{\\pi}}$\n",
    "\n",
    "    $\\Large{\\mathcal{v}_{\\pi}}\\normalsize{(s,a)} = \\Large{\\mathbb{E}}\\normalsize{( G_{t} | S_t = s, A_t = a )}$ \n",
    "    \n",
    "    $\\Large{\\mathcal{v}_{\\pi}}\\normalsize{(s,a)} = \\Large{\\mathbb{E} (}\\normalsize{\\sum\\limits_{k=0}^{\\infty}\\mathcal{\\gamma}^{k}{R_{t+k+1}}}|S_{t}=s, A_{t}=a\\Large{)} $<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611bc37",
   "metadata": {},
   "source": [
    "#### Optimal Policy and Optimal Value Function \n",
    "\n",
    "A policy $\\Large\\pi$ is better than ${\\Large{\\pi}}^{\\small{\\prime}}$\n",
    "if and only if $\\Large{\\mathcal{v}_{\\pi}} \\large(\\normalsize{s}\\large) > \\Large{\\mathcal{v}_{{\\pi}^{\\footnotesize{\\prime}}}}\\large(\\normalsize{s}\\large)$ &emsp; $\\large\\forall$ $ \\large s \\in S_t$ \n",
    "\n",
    "and a optimal value function or a $q$ is denoted as $\\large{\\mathcal{q}_{*}}$ <br>\n",
    "$\\Large{\\mathcal{q}_{*}} = \\normalsize{max_{\\pi}}$ $ \\Large{q_{\\normalsize{\\pi}}} \\normalsize{(a, s)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2edc42f",
   "metadata": {},
   "source": [
    "#### Bellman Optimality Equation for $q$ function \n",
    "$\\Large{\\mathcal{q}_{*}}\\large({s,a}) = \\Large{\\mathbb{E}}[\\large{R_{t+1}} \\normalsize{+} \\large{\\mathcal{\\gamma} }$ $ \\Large{max_{a ^\\prime}} $ $\\Large{q_{*}}\\large{(s^{\\footnotesize{\\prime}},a^{\\footnotesize{\\prime}})} \\Large{]}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46293f9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
