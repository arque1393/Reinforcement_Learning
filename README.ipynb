{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6cefce",
   "metadata": {},
   "source": [
    "$\\Huge{\\texttt{abcdefghijklmanopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34545442",
   "metadata": {},
   "source": [
    "> The true action value for action is the expected reward : \n",
    "$\\large{\n",
    "\\texttt{q}(a) = \\mathbb{E}[R_{t}|A_{t}=a]\n",
    "}$ <br>\n",
    "A simple estimate is the average of the sample reward :<br>\n",
    "$\\large{\n",
    "Q_{t}(a)=}\\Large{\n",
    "\\frac{\\sum\\limits_{n=1}^{t}R_{n}\\mathcal{I}(A_{n}= a)}{\\sum\\limits_{n=1}^{t}\\mathcal{I}(A_{n}= a)}\n",
    "}$\n",
    "          Where <br>$\\large{\\mathcal{I}(True)=1}$ and <br>\n",
    "          $\\large{\\mathcal{I}(False)=0}$\n",
    "          \n",
    "     \n",
    "> Update Incrementally: <br>\n",
    "$\\large{\n",
    "Q_t(A_t)=Q_{t-1}(A_t)+\\alpha_{t}(R_t - Q_{t-1}(A_t))}$,<br>\n",
    "$\\large{ \\forall{a}\\neq A_t : Q_t(a)=Q_{t-1}(a)\n",
    "}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c90b98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c28505ef",
   "metadata": {},
   "source": [
    "#### Tutorial Playlist : \n",
    "https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151ddb1",
   "metadata": {},
   "source": [
    "I an MDP we have $\\large\\mathcal{S}$ set of States $\\large\\mathcal{A}$ set of Actions $\\large\\mathcal{R}$ set of reawards<br>\n",
    "t reperesents the time stamp $t=0,1,2,3,...$ <br>\n",
    "where ${S}_{t}\\in \\large\\mathcal{S}_{t}$ and ${A}_{t} \\in \\large\\mathcal{A}_{t}$ and ${R}_{t}\\in \\large\\mathcal{R}_{t}$ <br>\n",
    "\n",
    "Now we can assume that $R_{t+1}$ is a function of $S_{t}$ and $A_{t}$ <br>\n",
    "$\\large\\mathcal{f}(S_t, A_t)=R_{t+1}$<br>\n",
    "###### History\n",
    "History is a sequence of Satate Action Reward\n",
    "$\\large\\mathcal{H}= S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3 .....$<br>\n",
    "![](resource/basicDiagram.png)\n",
    "\n",
    "#### Transition Probability \n",
    "$\\large\\mathcal{S}$and $\\large\\mathcal{R}$ are the finite set of *randomvariable* $S_{t}$ and $R_{t}$ that are must to be follow some well known *Probability Distyribution*.<br>\n",
    "And Probability of current time step is related to previous time steps\n",
    "\n",
    "Such as let $s^\\prime \\in \\large\\mathcal{S}$ and $r \\in \\large\\mathcal{R}$ then $\\mathcal{P}(S_{t}=s^\\prime)$ and $\\mathcal{P}(R_{t}=r)$ are depend on $a \\in \\mathcal{A}(s)$ <br>\n",
    "\n",
    "\n",
    "$\\Large\\forall$  ${s} \\in \\large\\mathcal{S}_{t}$ and \n",
    "${s^\\prime} \\in \\large\\mathcal{S}_{t}$ and ${a} \\in \\large\\mathcal{A}_{t}$ and ${r} \\in \\large\\mathcal{R}_{t}$\n",
    "\n",
    "$\\large\\mathcal{P}({s^\\prime},a | s, a )=\\large\\mathcal{P}\\{S_t=s^\\prime, R_t-r | S_{t-1}=s,A_{t-1}=a \\}$\n",
    "\n",
    "\n",
    "### Return \n",
    "Return is denoted by $\\large{G}$ for each time step $t$ , $\\large{G_t}$ <br>\n",
    "$\\large{G_t}$$={R_{t+1}}+{R_{t+2}}+{R_{t+3}}+{R_{t+4}}+.....+{R_{T}}$<br>\n",
    "Where $T$ is the final time step\n",
    "\n",
    "#### Discounted Return \n",
    "Discounted Return says that the return value are more dependent on currently collect reward and less depended on reward that are opccure comparetively more time ago\n",
    "\n",
    "$\\Large{G_t}$$\\large={R_{t+1}}+\\mathcal{\\gamma}{R_{t+2}}+\\mathcal{\\gamma}^2{R_{t+3}}+\\mathcal{\\gamma}^3{R_{t+4}}+.....$<br>\n",
    "\n",
    "$\\Large{G_t}$$=\\large{\\sum\\limits_{k=0}^{\\infty}\\mathcal{\\gamma}^{k}{R_{t+k+1}}}$\n",
    "\n",
    "$\\Large{G_t}$$=\\large{R_{t+1}+\\mathcal{\\gamma}{G_{t+1}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d9c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
