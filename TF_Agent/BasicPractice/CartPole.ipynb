{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8ee013-1914-4a67-9460-1585f3eab68f",
   "metadata": {},
   "source": [
    "# Hello World of Reinforcement Learning : Cart Pole "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a17dd-08ab-45d6-a3ad-14e42b4696be",
   "metadata": {},
   "source": [
    "### Install Dependency\n",
    "\n",
    "* tensorflow\n",
    "* tf_agents\n",
    "* gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9533be-9edd-4907-9bcf-0e210682b18c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from gym) (2.2.0)\n",
      "Requirement already satisfied: pyglet in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: tqdm in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (4.64.1)\n",
      "Requirement already satisfied: matplotlib in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (3.6.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: tensorflow in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (63.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (0.28.0)\n",
      "Requirement already satisfied: packaging in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (1.51.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (22.10.26)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: tf-agents in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: pillow in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (9.3.0)\n",
      "Requirement already satisfied: tensorflow-probability==0.17.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (0.17.0)\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (0.23.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (1.16.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (2.1.0)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (3.19.6)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (4.4.0)\n",
      "Requirement already satisfied: gin-config>=0.4.0 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (0.5.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (2.2.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tf-agents) (1.3.0)\n",
      "Requirement already satisfied: decorator in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow-probability==0.17.0->tf-agents) (5.1.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow-probability==0.17.0->tf-agents) (0.4.0)\n",
      "Requirement already satisfied: dm-tree in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from tensorflow-probability==0.17.0->tf-agents) (0.1.7)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/aritrarc/Program1/Reinforcement_Learning/TF_Agent/pyenv/lib/python3.10/site-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pyglet tqdm matplotlib\n",
    "!pip install tensorflow\n",
    "!pip install tf-agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bccbab6-ed21-4b88-b240-dd5ea18085bd",
   "metadata": {},
   "source": [
    "### Gym Cart Pole Environment setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5485b190-2b08-4a00-a86a-90db991899c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 08:02:49.753632: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 08:02:50.439323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-23 08:02:50.439350: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-23 08:02:51.851350: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 08:02:51.851427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 08:02:51.851437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fffe31f-69fc-4f5c-8aa9-51bfaf574862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "    def __init__(self, um_actions:int, num_hidden_units:int):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859aa1c6-7e47-440a-8954-1e629e6e693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 08:02:53.550834: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-23 08:02:53.551389: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-23 08:02:53.551432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (acr): /proc/driver/nvidia/version does not exist\n",
      "2022-11-23 08:02:53.552987: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1421973d-b7a9-453f-be41-1b82bd30f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap Gym's `env.step` call as an operation in a TensorFlow function.\n",
    "# This would allow it to be included in a callable TensorFlow graph.\n",
    "\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    return (state.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32))\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "    return tf.numpy_function(env_step, [action], [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0babead1-6821-4288-be8e-7f520bd8a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(initial_state:tf.Tensor,model: tf.keras.Model, \n",
    "            max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "    # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "\n",
    "    # Sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "\n",
    "    # Apply action to the environment to get next state and reward\n",
    "        state, reward, done = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "\n",
    "    # Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "\n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "\n",
    "    return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272b6aa1-35c7-4e89-b8f4-69a791ac2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards:tf.Tensor, gamma:float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # Start from the end of `rewards` and accumulate reward sums\n",
    "    # into the `returns` array\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "                   (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6a6ad35-fcac-4b71-8030-797915f6659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dbf78e8-1a63-44d9-a115-06bf67cc7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor,  model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float,  max_steps_per_episode: int) -> tf.Tensor:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Run the model for one episode to collect training data\n",
    "        action_probs, values, rewards = run_episode(\n",
    "            initial_state, model, max_steps_per_episode) \n",
    "\n",
    "        # Calculate the expected returns\n",
    "        returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "        # Convert training data to appropriate TF tensor shapes\n",
    "        action_probs, values, returns = [\n",
    "            tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "        # Calculate the loss values to update our network\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "        # Compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the model's parameters\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "293e6111-d021-4447-bd84-8623fd960903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                               | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:18\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "# `CartPole-v1` is considered solved if average reward is >= 475 over 500 \n",
    "# consecutive trials\n",
    "reward_threshold = 475\n",
    "running_reward = 0\n",
    "\n",
    "# The discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep the last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "t = tqdm.trange(max_episodes)\n",
    "for i in t:\n",
    "    initial_state, info = env.reset()\n",
    "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "    episode_reward = int(train_step(\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "    \n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "  \n",
    "\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "  \n",
    "    # Show the average episode reward every 10 episodes\n",
    "    if i % 10 == 0:\n",
    "        pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "  \n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "        break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307d10c-aa67-4eb2-8a1c-daa7e4492d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "\n",
    "render_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
    "    state, info = render_env.reset()\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "    screen = render_env.render()\n",
    "    images = [Image.fromarray(screen)]\n",
    "\n",
    "    for i in range(1, max_steps + 1):\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_probs, _ = model(state)\n",
    "        action = np.argmax(np.squeeze(action_probs))\n",
    "\n",
    "        state, reward, done, truncated, info = render_env.step(action)\n",
    "        state = tf.constant(state, dtype=tf.float32)\n",
    "\n",
    "        # Render screen every 10 steps\n",
    "        if i % 10 == 0:\n",
    "            screen = render_env.render()\n",
    "            images.append(Image.fromarray(screen))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images = render_episode(env, model, max_steps_per_episode)\n",
    "image_file = 'cartpole-v1.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(image_file, save_all=True, \n",
    "    append_images=images[1:],loop=0, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc668e-0266-4699-9964-dfb30d7e0d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecc8c9-a33d-479b-9db9-39c5399e5bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ab154-920a-4de0-a96d-67ee65173282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2bfbfd-90d0-4b76-902b-b7e0cdfed8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfaf29d-bb93-45a7-84fd-41ef0c07e4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f58fb6-e9f1-4677-a0fd-7fdaa4f5e502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8141c-bd3e-4e0d-8ae1-f6c89f06a121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44863dc-ec0d-4154-977e-54a1fa5e3e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507ff79-f3d6-4571-afb9-46e21c3837c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ae180-6f62-44a5-b201-00e90f538d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93074f2b-9e12-4e29-ac36-94c80f52b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inbuild environment of gym\n",
    "environment = gym.make('CartPole-v1')\n",
    "states = environment.observation_space.shape[0]\n",
    "actions = environment.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc542a1-a50e-4d30-9f4c-2155c43f8ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states,actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f637778-1ef2-4d80-a330-ede7e8c9bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1,    Score : 17.0\n",
      "Episode : 2,    Score : 20.0\n",
      "Episode : 3,    Score : 14.0\n",
      "Episode : 4,    Score : 22.0\n",
      "Episode : 5,    Score : 12.0\n",
      "Episode : 6,    Score : 15.0\n",
      "Episode : 7,    Score : 12.0\n",
      "Episode : 8,    Score : 19.0\n",
      "Episode : 9,    Score : 21.0\n",
      "Episode : 10,    Score : 23.0\n",
      "Episode : 11,    Score : 15.0\n",
      "Episode : 12,    Score : 10.0\n",
      "Episode : 13,    Score : 13.0\n",
      "Episode : 14,    Score : 18.0\n",
      "Episode : 15,    Score : 22.0\n",
      "Episode : 16,    Score : 25.0\n",
      "Episode : 17,    Score : 16.0\n",
      "Episode : 18,    Score : 35.0\n",
      "Episode : 19,    Score : 15.0\n",
      "Episode : 20,    Score : 10.0\n"
     ]
    }
   ],
   "source": [
    "no_episodes = 20\n",
    "for episode in range(1,no_episodes+1):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done :\n",
    "        environment.render()\n",
    "        action = random.choice([0,1])\n",
    "        n_state, reward, done , info = environment.step(action)\n",
    "        score+=reward \n",
    "    print(f\"Episode : {episode},    Score : {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282d4f7-8701-4e60-acc7-7eab68b3dcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
